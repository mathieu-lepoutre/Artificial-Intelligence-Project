---
title: "Artificial Intelligence Project"
author: "Mathieu Lepoutre"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Inleiding

De stad Antwerpen kampt met een langdurig probleem dat veel geld en politiemacht vergt om op te lossen. Spijtig genoeg valt dit probleem niet zomaar lossen, het is hetzelfde als dweilen met de kraan open.


Mijn paper gaat over het oplossen van het volgende probleem dmw machine learning. 
Illegale goederen worden binnengesmokkeld via de Antwerpse haven. Een deel van het gigantisch aantal containers dat per jaar binnekomt wordt doorgestuurd naar scanners. Deze zitten constant op volle capaciteit.

Wat als we beter kunnen inschatten welke containers positief zullen zijn ?
Via machine learning kunnen we modellen maken die op basis van  data kunnen voorspellen of een container illegale goederen bevat.

Enkele eigenschappen van de Haven van Antweren;
[Haven van Antwerpen Feiten en Cijfers]https://www.portofantwerp.com/sites/portofantwerp/files/Feiten_en_Cijfers_2019.pdf


```{r }
library(readr)
library(readr)
boten <- read.csv( file ="~/PositionBoats.csv" , header = TRUE, sep = ";")

View(boten)
str(boten)

summary(boten)

boten

boten$AISTrajectPositionLongitude <- as.double(boten$AISTrajectPositionLongitude)

boten$AISTrajectPositionSpeedOverGround <- as.numeric(boten$AISTrajectPositionSpeedOverGround)

```

##Inhoudstafel

# Project analyse:

- Analyse van de probleemstelling
- Scheiden van afhankelijke en onafhankelijke variabelen
- Vastleggen performantie criteria
- Maak beslissingen rond levenscyclus


# Project uitvoering:

- Verzamelen van datasets
- Filteren, opkuisen en samenvoegen
- Randomiseren
- Trainen leeralgoritme
- Instellen hyperparameters
- Validatie
- Testen

```{r }
df
kable(df)
```

### Machine learning
 
Supervised machine learning
  - regression (linear, polynomial)
  - decision tree
  - Random Forest
  
  
  -KNN : k-Nearest Neighbors
  -Trees :  Classification and Regression Trees
  -Logistic regression
  -Naive-Bayes
  -SVM : Support Vector Machines 
  - LDA Linear Discriminant Analysis
  

  
  
  
```{r}

```
  
## Model 1: Kalman filter
### predictive analytics

** -> voorspellen of een inbreng (persoon, container) een bepaalde waarde heeft of niet, gebaseerd op vorige data.

het bouwen van een voorspellings model dat een waarde klassifieerd als x of y gebasseerd op data
roadmap -> importeren data :(readcsv, str, eventueel kolommen hernoemen)
            data opkuisen :(redundante kolommen eruit halen, data converteren in numeriek formaat, kolommen met missende data aanvullen, kolommen transformeren)
            model opbouwen :data splitten (trainingset (hier hoger aantal als testset) en testset)
            model trainen :(algoritmes toepassen zoals knn, ) op beide toepassen
            model testen :table met testdata en echte data en zien of het klopt
            efficientie verhogen!
            
            
            
          
            
            
```{r}

```
  

## Model 1: RNN 
### Recuring Neural Network


** Voor het voorspellen van een tijdseries adhv een recurrent neural network, maak ik gebruik van een Multivariate Time Series Forecasting with LSTMs.
De volgende code is geschreven op basis van een r-studio blog.
https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/
            
```{r}
data <- read.csv("~/GitHub/rnn/DataShipPositions.csv")

 

 
 str(data)

 summary(data)
 
 ggplot(data, aes(x=1:nrow(data),y=AISTrajectPositionLongitude)) + geom_line()
 ggplot(data, aes(x=1:nrow(data),y=AISTrajectPositionLatitude)) + geom_line()
 
 data <- data.matrix(data[,-1])
 
glimpse(data)

ggplot(data[1:100,], aes(x=1:100,y=AISTrajectPositionLongitude)) + geom_line()

##given data going as far back as lookback timesteps (a timestep is 10 minutes) 
##and sampled every steps timesteps, can you predict the temperature in delay timesteps? Youâ€™ll use the following parameter values:
  

#lookback = 100
#steps =3
#delay = 10


data <- data.matrix(data[,-1])

train_data <- data[1:100,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)




generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 5, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }           
    list(samples, targets)
  }
}






lookback <- 100
step <- 3
delay <- 100
batch_size <- 5



train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 100,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 101,
  max_index = 180,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 181,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

val_steps <- (181 - 101 - lookback) / batch_size
test_steps <- (nrow(data) - 181 - lookback) / batch_size


model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)



model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)


history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 50,
  epochs = 2,
  validation_data = val_gen,
  validation_steps = val_steps
)

```

### Data Visualizatie

```{r}

```


